#!/usr/bin/env python3
# This is a analyzer of the results generated by theoracle.py and timeddns.py
# This program i standalone and does not require the above mentioned files

import sys
import os
import logging
import csv
import matplotlib.pyplot as plt
import time
import collections
import multiprocessing

#
#   LOGGER
#

log_format = "%(asctime)s %(name)s [%(levelname)s] %(message)s" # From exitmap by Philipp Winter
logging.basicConfig(format=log_format, level=logging.DEBUG)
log = logging.getLogger("analyzer")

#
#   LOAD DATA FROM SYSTEM
#

def read_csv(path):
    data = []
    with open(path, mode='r') as csv_file:
        for line in csv.DictReader(csv_file):
            data.append(line)
    return data

def load_data_from_result_files(files):
    log.info("Found " + str(len(files)) + " csv files. Loading datapoints, please wait...")
    data = []
    for f in files:
        data.extend(read_csv(f))
    return data

def find_result_files(path):
    files = []
    for path in [f.path for f in os.scandir(path) if f.is_dir()]:
        files.extend([f.path for f in os.scandir(path) if f.is_file()])
    return files 

#
#   STATS
#

def datapoints_count(data):
    return int(len(data))

def datapoints_successfull(data):
    return [x for x in data if float(x["time"]) > 0 ]

def datapoints_successfull_count(data):
    return int(len(datapoints_successfull(data)))

def datapoints_failed(data):
    return [x for x in data if float(x["time"]) <= 0 ]

def datapoints_failed_count(data):
    return int(len(datapoints_failed(data)))

def directories(data):
    return list(set([x["directory"] for x in data]))

def directories_count(data):
    return int(len(directories(data)))

def exits(data):
    return list(set([x["fingerprint"] for x in data]))

def exits_count(data):
    return int(len(exits(data)))

#
#   LIST
#

def is_domain_unique(data, domain):
    return is_unique([x["domain"] for x in data], domain)

def is_unique(data, element):
    return (len(filter(data, element)) == 1)

def filter(data, element):
    return [x for x in data if x == element]

#
#   ANALYZE
#

# Reformats the dataset according to new format with more information
def analyze_format_dataset(datapoint, cached):
    a = {}
    a["directory"]      = datapoint["directory"]
    a["fingerprint"]    = datapoint["fingerprint"]
    a["domain"]         = datapoint["domain"]
    a["time"]           = datapoint["time"]
    a["timestamp"]      = datapoint["timestamp"]
    a["cached"]         = str(cached)
    return a

# Selects/filters datapoints according to directory
def data_by_directory(data, directory):
    return [x for x in data if x["directory"] == directory]

# Selects/filters datapoints according to multiple directories
def data_by_directories(data, directories):
    output = []
    [output.extend(data_by_directory(data, d)) for d in directories]
    return output

# Provide list dataset and single datapoint.
# Returns True if datapoint is the last datapoint of that domian in dataset.
# Else return False.
def is_last(dataset, datapoint_is_last):
    for d in data_by_directory(data, datapoint_is_last["directory"]): # Filter by directory, separate scans should not interfeer
        if (d["domain"] == datapoint_is_last["domain"]): # Filter by domain
            if (float(datapoint_is_last["timestamp"]) > float(d["timestamp"])): # If datapoint came before or after another
                return True
    return False

# Formats dataset according to analyze_format_dataset() and removes caching resolves.
def analyze_select(data, procnum, return_dict):
    output = []
    l = str(len(data))
    i = 0
    output = []
    for d in data:
        # Process dataset
        dbd = data_by_directory(data, d["directory"])
        if is_domain_unique(dbd, d["domain"]):
            # NON CACHED = Domain is unique
            #
            # Selects all non cached resolves from a greater list, will append "cached" attribute as False to result.
            # Non cached are those which in the scop of a directory and a fingerprint are only resolved once.
            output.append(analyze_format_dataset(d, False))
            #log.debug("Found non cached." + str(d["domain"]) + " " + str(d["time"]))
        elif is_last(dbd, d):
            # PRE CACHED = Domain is not unique and is last in its direcory
            #
            # Selects all pre cached resolves from a greater list, will append "cached" attribute as True to result.
            # Cached are those results which in the scope of a directory and a fingerprint resolve the same domain twice.
            # Out of the two resolves, only the second is selected since the first is only for caching in the exit node.
            output.append(analyze_format_dataset(d, True))
            #log.debug("Found last pre cached." + str(d["domain"]) + " " + str(d["time"]))
        #else:
            # This will trigger when the first resolve of a pre cache resolve is found.
            #log.debug("Found nothing " + str(d["domain"]) + " " + str(d["time"]))
        
        # Debug
        i += 1
        if (i % 100 == 0):
            log.debug("Analyze pre/non cached " + str(i) + " out of " + l + " done in process num #" + str(procnum) + ".")
    return_dict[procnum] = output
    return True

def analyze_multithread(data):
    dirs = directories(data) # Directories
    procs = [] # Processes
    processes = int(multiprocessing.cpu_count() - 2) # One cpu left for system reserve and one for leftover data due to roudning error
    if (processes > len(dirs)):
        processes = len(dirs)
    log.debug("Number of processes to use " + str(processes) + " ( + 1 for picking up slack).")

    dirs_size_per_process = int(len(dirs) / processes)
    log.debug("Directory size for each process " + str(dirs_size_per_process) + " out of " + str(len(dirs)) + " total directories")    

    # Multi process manager
    return_dict = multiprocessing.Manager().dict()

    process_data_list = []
    for _ in range(processes):
        log.debug("Adding data from " + str(len(dirs)) + " directories.")
        for _ in range(dirs_size_per_process):
            process_data_list.append(data_by_directory(data, dirs.pop()))
    # Pick up slack
    if len(dirs) == 0:
        log.debug("No slack data left.")
    else:
        log.debug("Adding data from " + str(len(dirs)) + " directories (to slack process).")
        while (len(dirs) > 0):
            process_data_list.append(data_by_directories(data, dirs))

    # Allocate data to processes
    for pd in process_data_list:
        procs.append(multiprocessing.Process(target=analyze_select, args=(list(pd), len(procs), return_dict)))
        log.debug("Process added with " + str(len(pd)) + " lines of data from " + str(directories_count(pd)) + " directories.")

    # Start processes
    for proc in procs:
        proc.start()

    # Join processes
    output = []
    for proc in procs:
        proc.join()
        # Combine process output
        for value in return_dict.values():
            log.debug("Process returned  " + str(len(value)) + " values.")
            output.extend(value)
            log.info("Output currently have " + str(len(output)) + " values, " + str(len([x for x in output if str(x["cached"]) == str("True")])) + " cached and " + str(len([x for x in output if str(x["cached"]) == str("False")])) + " non cached.")
    return output

def write_data(data, filename):
    with open(str(filename + ".csv"), mode='w') as F:
        csv_writer = csv.DictWriter(F, fieldnames=['directory', 'fingerprint', 'domain', 'time', 'timestamp', 'cached'])
        csv_writer.writeheader()
        csv_writer = csv.writer(F)
        for row in data:
            csv_writer.writerow([
                row['directory'],
                row['fingerprint'],
                row['domain'],
                row['time'],
                row['timestamp'],
                row['cached']
                ])

def analyze(data):
    log.info("Datapoints\t\t\t" + str(datapoints_count(data)))
    log.info("Sucessfull resolves\t\t" + str(datapoints_successfull_count(data)))
    log.info("Failed resolves\t\t\t" + str(datapoints_failed_count(data)))
    log.info("Directories (exitmap runs)\t" + str(directories_count(data)))
    log.info("Fingerprints (exits scanned)\t" + str(exits_count(data)))

    # Aggregate and analyze data
    t = time.time()
    agg_data = analyze_multithread(data)
    log.debug("Aggregated and analyzed in " + str(float(time.time() - t)) + " seconds.")

    # Validate data
    # If a domain in the original dataset does not exist in the aggregated data
    log.info("Validating data, please wait...")
    t = time.time()
    i = 0
    for d in data:
        if i % 1000 == 0:
            log.debug("At " + str(i))
        if not d["domain"] in [x["domain"] for x in agg_data]:
            log.warning("Original domain " + str(d["domain"]) + " does not exist in aggregated data!")
        i += 1
    log.debug("Validated aggregated dataset in " + str(float(time.time() - t)) + " seconds.")

    return agg_data

#
#   BOOSTRAP ANALYZER
#

def remove_blacklisted(data, keyword):
    return [x for x in data if not str(keyword) in str(x)]

if __name__ == "__main__":
    # Check input directory
    try:
        input_directory = sys.argv[1]
        log.debug("Input directory: " + str(input_directory))
        if not os.path.isdir(input_directory):
            log.error("Input directory not valid.")
            exit(-1)
    except Exception as ex:
        log.error("Exception input directory " + str(ex))
        log.error("Supply a input directory with results from exitmap. syntax: python program input-directory keyword output-directory")
        exit(-1)
    
    # Check blacklist keyword
    try:
        blacklist_keyword = sys.argv[2]
        log.debug("Domain blacklist keyword: " + str(blacklist_keyword))
    except Exception as ex:
        log.error("Exception input " + str(ex))
        log.error("Supply a keyword for blacklisting of domains, ex: keyword oscar will remove oscaradawdada.com. syntax: python program input-directory keyword output-directory")
        exit(-1)

    # Check output directory
    try:
        output_directory = sys.argv[3]
        log.debug("Output directory: " + str(output_directory))
        if not os.path.isdir(output_directory):
            log.error("Output directory not valid.")
            exit(-1)
    except Exception as ex:
        log.error("Exception output directory " + str(ex))
        log.error("Supply a directory for output file. syntax: python program input-directory keyword output-directory")
        exit(-1)

    # Cleanup data
    t = time.time()
    data = load_data_from_result_files(find_result_files(input_directory))
    l = int(len(data))
    log.debug("Loaded " + str(l) + " datapoints in " + str(float(time.time() - t)) + " seconds.")
    data = remove_blacklisted(data, blacklist_keyword)
    log.debug("Removed according to blacklist keyword " + str(blacklist_keyword) + " " + str(len(data)) + " datapoints remaining, " + str(int(l - len(data))) + " removed.")
    t = time.time()

    # Analyze and write dataset
    data = analyze(data)
    log.info("Writing data to disk, please wait...")
    write_data(data, str(output_directory + "/theoracle_" + str(time.time())))
    log.debug("Analyzed data in " + str(float(time.time() - t)) + " seconds.")

    # Program quit successfully
    log.info("Analyzer done.")
    exit(0)
