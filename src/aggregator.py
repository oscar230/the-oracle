#!/usr/bin/env python3
# This is a aggregtor of the results generated by theoracle.py and timeddns.py
# This program is standalone and does not require the above mentioned files

import sys
import os
import logging
import csv
import time
import collections
import multiprocessing

#
#   LOGGER
#

log_format = "%(asctime)s %(name)s [%(levelname)s] %(message)s" # From exitmap by Philipp Winter
logging.basicConfig(format=log_format, level=logging.DEBUG)
log = logging.getLogger("aggregator")

#
#   LOAD AND SAVE DATA
#

def read_csv(path):
    data = []
    with open(path, mode='r') as csv_file:
        for line in csv.DictReader(csv_file):
            data.append(line)
    return data

def load_data_from_result_files(files):
    log.info("Found " + str(len(files)) + " csv files. Loading datapoints, please wait...")
    data = []
    for f in files:
        data.extend(read_csv(f))
    return data

def find_result_files(path):
    files = []
    for path in [f.path for f in os.scandir(path) if f.is_dir()]:
        files.extend([f.path for f in os.scandir(path) if f.is_file()])
    return files 

def write_data(data, filename):
    with open(str(filename + ".csv"), mode='w') as F:
        csv_writer = csv.DictWriter(F, fieldnames=['directory', 'fingerprint', 'domain', 'time', 'timestamp', 'cached'])
        csv_writer.writeheader()
        csv_writer = csv.writer(F)
        for row in data:
            csv_writer.writerow([
                row['directory'],
                row['fingerprint'],
                row['domain'],
                row['time'],
                row['timestamp'],
                row['cached']
                ])

#
#   MULTIPROCESSING
#

def mp_get_processors():
    return int(multiprocessing.cpu_count())

def mp_aggreator(data_by_fingerprint):
    log.debug("Multiprocessing aggregator got " + str(len(data_by_fingerprint)) + " rows of data.")
    
    return_dict = multiprocessing.Manager().dict()

    # Setup processes
    processes_waiting_list = list()
    t = time.time()

    try:
        for dbf in sorted(data_by_fingerprint, key=lambda d: len(d), reverse=False): # Sort by size to make the fastest process join first
            #log.debug(str(len(dbd)))
            processes_waiting_list.append(multiprocessing.Process(target=agg_main, args=(list(dbf), len(processes_waiting_list), return_dict)))
        log.debug("Added " + str(len(processes_waiting_list)) + " processes to waiting list.")
    except Exception as ex:
        log.error("Setup processes error " + str(ex))
        exit(-1)

    processes_running_list = list()
    # If either list is not empty
    while len(processes_waiting_list) > 0 or len(processes_running_list) > 0:
        while len(processes_running_list) < mp_get_processors() and len(processes_waiting_list) > 0:
            # Running list is not full and waiting list is not empty.
            # Add more process.
            processes_running_list.append(processes_waiting_list.pop())
            processes_running_list[len(processes_running_list) - 1].start() # Start last process in list

        while len(processes_running_list) > 0:
            # Running list is full or waiting list is empty
            # Runnign list cannot be empty
            # Wait for processes to join.
            try:
                p = processes_running_list.pop()
                p.join()
            except Exception as ex:
                log.error("\u001b[31mCould not join process, error " + str(ex) + "\u001b[0m")
            else:
                log.debug("Process joined the main process.")
         # Stats printout
        if time.time() - t > 10:
            t = time.time()
            log.info("Aggregator running, " + str(len(processes_running_list) + len(processes_waiting_list)) + " processes left.")

    # Handle output
    output = []
    [output.extend(value) for value in return_dict.values()]
    log.debug("Multiprocessing aggregator returning " + str(len(output)) + " rows of data.")
    return output

#
#   AGGREGATOR
#

# Reformats the dataset according to new format with more information
def agg_format(datapoint, cached):
    a = {}
    a["directory"]      = datapoint["directory"]
    a["fingerprint"]    = datapoint["fingerprint"]
    a["domain"]         = datapoint["domain"]
    a["time"]           = datapoint["time"]
    a["timestamp"]      = datapoint["timestamp"]
    a["cached"]         = str(cached)
    return a

# Is datapoint in data set cached
# Returns True or False
def agg_is_cached(data, datapoint):
    # Domain is not unique and must be cached in at least once measurement
    return len([x for x in data if x["domain"] == datapoint["domain"]]) > 1

# Is datapoint the last of its domain in the data set
# Presumption that the datapoint is not unique in the data set
# Returns True or False
def agg_is_last(data, is_this_datapoint_last):
    for current_datapoint in data:
        # Find domain
        if is_this_datapoint_last["domain"] == current_datapoint["domain"]:
            # Compare timestamp
            if float(is_this_datapoint_last["timestamp"]) > float(current_datapoint["timestamp"]):
                return True
    return False
    # log.error("Aggregator is last operation performed on a unique datapoint, exiting.")
    # exit(-1)

# Main aggreagtor function
# Capable of multi processing
# Returns format seen in agg_format(...)
def agg_main(main_data, procnum=0, return_dict=list()):
    log.info("\u001b[36mAggregator #" + str(procnum) + " starting with " + str(len(main_data)) + " rows of data, please wait...\u001b[0m")
    output = [] # Where resulting output is put, then returned when the function is finished
    t0 = time.time() # Total running time

    for data_by_directory in agg_devide_data_by_directory(main_data): # Divide by directory and go trough
        for datapoint in data_by_directory: # Go trough every datapoint in the directory
            is_cached = agg_is_cached(data_by_directory, datapoint)
            if (not is_cached) or (is_cached and agg_is_last(data_by_directory, datapoint)):
                output.append(agg_format(datapoint, is_cached))

    log.info("\u001b[32mAggregator #" + str(procnum) + " finished in " + str(float(time.time() - t0))[0:5] + " seconds, returning " + str(len(output)) + " rows of data.\u001b[0m")
    return_dict[procnum] = output # Return resulting data to main process
    return True

# Devide data in terms of thier exit node (fingerprint)
# Returns list in list
def agg_devide_data_by_fingerprint(data):
    fingerprints = set([x["fingerprint"] for x in data])
    data_by_fingerprint = []
    for fingerprint in fingerprints:
        data_by_fingerprint.append([x for x in data if x["fingerprint"] == fingerprint])
    return data_by_fingerprint

# Devides data in terms of thier directory
# Returns lists in list
def agg_devide_data_by_directory(data):
    directories = set([x["directory"] for x in data])
    data_by_directory = []
    for directory in directories:
        data_by_directory.append([x for x in data if x["directory"] == directory])
    return data_by_directory

# Removes all datapoints where keyword exists in domain
def agg_remove_blacklist(data, keyword):
    log.debug("Will remove rows by keyword " + str(keyword) + " from " + str(len(data)) + " rows of data.")
    return [x for x in data if not str(keyword) in str(x["domain"])]

#
#   BOOTSTRAP
#

if __name__ == "__main__":
    try:
        # Check input directory
        input_directory = sys.argv[1]
        log.debug("Input directory: " + str(input_directory))
        if not os.path.isdir(input_directory):
            log.error("Input directory not valid.")
            exit(-1)
        # Check blacklist keyword
        blacklist_keyword = sys.argv[2]
        log.debug("Domain blacklist keyword: " + str(blacklist_keyword))
    except Exception as ex:
        log.error("Command line argument error " + str(ex))
        log.info("Syntax: " + str(sys.argv[0]) + " input-directory blacklisted-domain")
        log.info("The input directory contains data genreated by theoracle.py, the blacklisted domain will remove all domains containing that string in the data set provided as input.")
        exit(-1)
    else:
        try:
            # Load data
            t = time.time()
            log.info("Loading data, please wait...")
            data = load_data_from_result_files(find_result_files(input_directory))
            log.info("Loaded " + str(len(data)) + " rows of data in " + str(float(time.time() - t))[0:5] + " seconds from " + str(input_directory) + " directory.")
        except Exception as ex:
            log.error("Load data from CSV error " + str(ex))

        try:
            # Remove blacklisted data
            t = time.time()
            log.info("Removing data according to blacklist, please wait...")
            data = agg_remove_blacklist(data, blacklist_keyword)
            log.info("Removed data according to blacklisted keyword " + str(blacklist_keyword) + " in " + str(float(time.time() - t))[0:5] + " seconds, resulting in " + str(len(data)) + " rows of data left.")
        except Exception as ex:
            log.error("Blacklist operation error " + str(ex))

        try:
            # Statistics
            log.info("Data containing " + str(len(set([x["directory"] for x in data]))) + " directories.")
            log.info("Aggregator should result in ~" + str(int(len(data) - len(data) / 3)) + " rows of data.")
        except Exception as ex:
            log.error("Statistics error " + str(ex))

        # Run main aggregator
        t = time.time()
        log.info("Aggregating, please wait...")
        data = mp_aggreator(agg_devide_data_by_fingerprint(data))
        log.info("Aggregator done in " + str(float(time.time() - t))[0:5] + " seconds.")

        # Sorting data set
        try:
            data_tmp = sorted(data, key=lambda d: float(d["timestamp"]), reverse=False)
        except Exception as ex:
            log.error("Data soring error " + str(ex))
            log.info("Data sorting failed, will save unsorted data instead")
        else:
            log.debug(str(len(data_tmp)) + " rows of data sorted by timestamp sucessfully.")
            data = data_tmp

        # Saving to disk
        t = time.time()
        try:
            # Write data to disk
            write_data(data, "theoracle_aggregated_" + str(time.time()))
        except Exception as ex:
            log.error("Write data error " + str(ex))
        else:
            log.info("Wrote data to disk in " + str(float(time.time() - t))[0:5] + " seconds.")

    finally:
        # Exit
        log.info(str(sys.argv[0]) + " exited normally.")
        exit(0)